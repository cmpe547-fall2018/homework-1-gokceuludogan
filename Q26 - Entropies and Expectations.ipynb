{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q26 - Entropies and Expectations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expectation of a function of a discrete random variable is denoted as \n",
    "\n",
    "$$\\langle f(x)\\rangle \\equiv \\sum_{x \\in X} f(x)p(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, for a pair of random variables, we have the expectation\n",
    "\n",
    "$$ \\langle f(x, y) \\rangle \\equiv \\sum_{x \\in X} \\sum_{y \\in Y} f(x,y)p(x,y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance is defined as \n",
    "\n",
    "$$ Var\\{f(x)\\} = \\langle {(f(x) - \\langle f(x) \\rangle)}^{2} \\rangle $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a measure of spread. For a pair of random variables, the covariance is \n",
    "\n",
    "$$ Cov[f(x), g(y)] = \\langle (f(x) - \\langle f(x) \\rangle ) (g(y) - \\langle g(y) \\rangle ) \\rangle$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance gives the information about the dependence between $ f(x) $ and $ g(y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a probability table $p(x,y)$ specified as a matrix and respective domains of two discrete variable $ x \\in X $ and $ y \\in Y$,  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectations $\\langle x \\rangle$, $\\langle y \\rangle$, $\\langle y | x \\rangle$, $\\langle x | y \\rangle$, $Cov[x,y]$ are below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal_probability(joint_probability, axis):\n",
    "    '''\n",
    "    Gets marginal probability given joint probabilities and axis for summing probabilities\n",
    "    axis should be 1 for variable X to sum the joint probability table over rows\n",
    "    axis should be 0 for variable Y to sum the joint probability table over columns  \n",
    "    '''\n",
    "    return np.sum(joint_probability, axis=axis).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation_of_variable(joint_probability, variable, axis):\n",
    "    '''\n",
    "    Calculates expectation of variable given joint probabilities, values of variable and axis for marginal probability \n",
    "    axis should be 1 for variable X to sum the joint probability table over rows\n",
    "    axis should be 0 for variable Y to sum the joint probability table over columns    \n",
    "    '''\n",
    "    marginal_probability = np.sum(joint_probability, axis=axis).flatten()\n",
    "    return np.asscalar(np.dot(marginal_probability, np.transpose(variable)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_probability(joint_probability, axis):\n",
    "    '''\n",
    "    Calculates conditional probability given joint probabilities and axis for computing marginal probabilities \n",
    "    '''\n",
    "    if axis == 1:\n",
    "        return np.transpose(np.transpose(joint_probability) / marginal_probability(joint_probability, axis))        \n",
    "    else:\n",
    "        return joint_probability / marginal_probability(joint_probability, axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation_of_pair(probability_table, X, Y):\n",
    "    '''\n",
    "    Computes the expectation of pair given values of variables and their probability distribution\n",
    "    '''\n",
    "    expectation = 0\n",
    "    for ind_x, x in enumerate(X):\n",
    "        for ind_y, y in enumerate(Y):\n",
    "            expectation += x * y * probability_table[ind_x,ind_y]\n",
    "    return expectation        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance(joint_probability, X, Y):\n",
    "    '''\n",
    "    Calculates the covariance of given variables for given probability distribution\n",
    "    '''\n",
    "    expectation = 0\n",
    "    for ind_x, x in enumerate(X):\n",
    "        for ind_y, y in enumerate(Y):\n",
    "            expectation += (x - expectation_of_variable(joint_probability, X, 1)) * (y - expectation_of_variable(joint_probability, Y, 0)) * joint_probability[ind_x,ind_y]\n",
    "    return expectation        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(x):\n",
    "    '''\n",
    "    Sets 0 to -inf in the log of given variable\n",
    "    '''\n",
    "    temp_x = np.copy(x)\n",
    "    temp_x[x == 0] = 1\n",
    "    return np.log(temp_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joint Entropy\n",
    "$$ H[x,y] = - {\\langle logp(x,y) \\rangle}_{p(x,y)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(probability_table):\n",
    "    '''\n",
    "    Finds entropy for given probability table\n",
    "    '''\n",
    "    log_of_probability_table = log(probability_table)\n",
    "    return -np.sum(-np.multiply(probability_table, log_of_probability_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marginal Entropies\n",
    "\n",
    "$$ H[x] = - {\\langle logp(x) \\rangle}_{p(x)}$$\n",
    "$$ H[y] = - {\\langle logp(y) \\rangle}_{p(y)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal_entropy(joint_probability, axis):\n",
    "    marginal_prob = marginal_probability(joint_probability, axis)\n",
    "    log_of_marginal_prob = log(marginal_prob)\n",
    "    return -np.sum(-np.multiply(marginal_prob, log_of_marginal_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional Entropies\n",
    "\n",
    "\n",
    "$$ H[y|x] = - {\\langle logp(y|x) \\rangle}_{p(x,y)}$$\n",
    "$$ H[x|y] = - {\\langle logp(x|y) \\rangle}_{p(x,y)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy(joint_probability, given_variable):\n",
    "    cond_prob = conditional_probability(joint_probability, given_variable)\n",
    "    log_cond_prob = log(cond_prob)\n",
    "    return -np.sum(-np.multiply(joint_probability, log_cond_prob))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual Information\n",
    "\n",
    "$$ I(x,y) = H[x] - H[x|y] = KL(p(x,y)||p(x)p(y)) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information(joint_probability):\n",
    "    # equal to H[X] - H[X|Y]\n",
    "    return marginal_entropy(joint_probability, 0) - conditional_entropy(joint_probability, 1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculations for given joint probability table:\n",
    "\n",
    "| p(x,y) \t| y = -1 \t| y = 0 \t| y = 5 \t|\n",
    "|--------\t|--------\t|-------\t|-------\t|\n",
    "| x = 1  \t| 0.3    \t| 0.3   \t| 0     \t|\n",
    "| x = 2  \t| 0.1    \t| 0.2   \t| 0.1   \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = [1, 2]\n",
    "Y = [-1, 0, 5]\n",
    "joint_probability = np.matrix([[0.3, 0.3, 0], [0.1, 0.2 , 0.1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPECTATIONS\n",
      "========================\n",
      "Expectation of x:\n",
      "1.40\n",
      "Expectation of y:\n",
      "0.10\n",
      "Expectation of x given y:\n",
      "1.50\n",
      "Expectation of y given x:\n",
      "8.75\n",
      "Covariance of x, y:\n",
      "0.36\n"
     ]
    }
   ],
   "source": [
    "print('EXPECTATIONS')\n",
    "print('========================')\n",
    "print('Expectation of x:')\n",
    "print('{:.2f}'.format(expectation_of_variable(joint_probability, X, 1)))\n",
    "print('Expectation of y:')\n",
    "print('{:.2f}'.format(expectation_of_variable(joint_probability, Y, 0)))\n",
    "\n",
    "print('Expectation of x given y:')\n",
    "print('{:.2f}'.format(expectation_of_pair(conditional_probability(joint_probability, 1), X, Y)))\n",
    "print('Expectation of y given x:')\n",
    "print('{:.2f}'.format(expectation_of_pair(conditional_probability(joint_probability, 0), X, Y)))\n",
    "\n",
    "print('Covariance of x, y:')\n",
    "print(\"{:.2f}\".format(covariance(joint_probability, X, Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTROPIES\n",
      "=====================\n",
      "Joint entropy H(X,Y):\n",
      "-1.50\n",
      "Marginal Entropies\n",
      "H[x]: -0.67\n",
      "H[y]: -0.94\n",
      "Conditional Entropies\n",
      "H[x|y]: -0.56\n",
      "H[y|x]: -0.83\n",
      "Mutual Information I(X;Y):\n",
      "-0.11\n"
     ]
    }
   ],
   "source": [
    "print('ENTROPIES')\n",
    "print('=====================')\n",
    "print('Joint entropy H(X,Y):')\n",
    "joint_entropy = entropy(joint_probability)\n",
    "print('{:.2f}'.format(joint_entropy))\n",
    "print('Marginal Entropies')\n",
    "h_x = marginal_entropy(joint_probability, 1)  \n",
    "h_y = marginal_entropy(joint_probability, 0)      \n",
    "print('H[x]: ' + '{:.2f}'.format(h_x))\n",
    "print('H[y]: ' + '{:.2f}'.format(h_y))\n",
    "\n",
    "print('Conditional Entropies')\n",
    "h_x_given_y = conditional_entropy(joint_probability, 0)  \n",
    "h_y_given_x = conditional_entropy(joint_probability, 1)      \n",
    "print('H[x|y]: ' + '{:.2f}'.format(h_x_given_y))\n",
    "print('H[y|x]: ' + '{:.2f}'.format(h_y_given_x))\n",
    "\n",
    "print('Mutual Information I(X;Y):')\n",
    "mutual_info = mutual_information(joint_probability)      \n",
    "print('{:.2f}'.format(mutual_info)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify following equations illustrated in the picture below.\n",
    "\n",
    "$$ H(X,Y) = H(X) + H(Y|X) = H(X|Y) + H(Y) $$\n",
    "$$ I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) $$ \n",
    "$$ H(X,Y) = I(X;Y) + H(X|Y) + H(Y|X) $$\n",
    "\n",
    "\n",
    "![Entropy](Entropy.png \"Entropy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert '{:.2f}'.format(joint_entropy) == '{:.2f}'.format(h_x + h_y_given_x) \n",
    "assert '{:.2f}'.format(joint_entropy) == '{:.2f}'.format(h_y + h_x_given_y)\n",
    "assert '{:.2f}'.format(mutual_info) == '{:.2f}'.format(h_x - h_x_given_y) \n",
    "assert '{:.2f}'.format(mutual_info) == '{:.2f}'.format(h_y - h_y_given_x) \n",
    "assert '{:.2f}'.format(joint_entropy) == '{:.2f}'.format(mutual_info + h_x_given_y + h_y_given_x) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
